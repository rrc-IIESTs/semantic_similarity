{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4ox2V3iuNwpf",
        "outputId": "46499e66-d519-4c9a-a2f6-e4d8f4c49a5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This code is used to ganarate the data set from text file\\n    it gives the output as fill in the blanks questions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"This code is used to ganarate the data set from text file\n",
        "    it gives the output as fill in the blanks questions\n",
        "    This code generates a csv file \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code for generating the data set from txt file .\n",
        "\n",
        "import random\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "def is_special_character(word):\n",
        "    # Check if the word contains only special characters\n",
        "    return all(char in string.punctuation for char in word)\n",
        "\n",
        "def remove_specific_tokens(sentence, tokens):\n",
        "    for token in tokens:\n",
        "        sentence = sentence.replace(token, '')\n",
        "    return sentence.strip()\n",
        "\n",
        "def randomly_mask_one_word(sentence, mask_string='[MASK]', tokens_to_remove=['<unk>', '<eob>']):\n",
        "    sentence = remove_specific_tokens(sentence, tokens_to_remove)\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Filter out words that are special characters\n",
        "    non_special_words = [word for word in words if not is_special_character(word)]\n",
        "\n",
        "    if len(non_special_words) > 3:  # Sentence length should be more than 3\n",
        "        word_to_mask_index = random.randint(0, len(non_special_words) - 1)\n",
        "        masked_sentence = ' '.join(mask_string if word == non_special_words[word_to_mask_index] else word for word in words)\n",
        "        masked_word = non_special_words[word_to_mask_index]\n",
        "        return masked_sentence, masked_word\n",
        "    else:\n",
        "        # If sentence length is less than or equal to 3, return None\n",
        "        return None, None\n",
        "\n",
        "def process_text_file(input_file, output_file):\n",
        "    # Read input from text file\n",
        "    with open(input_file, 'r') as file:\n",
        "        sentences = [sentence.strip().replace('\"', '') for sentence in file.readlines() if sentence.strip() and len(sentence.split()) > 3]\n",
        "\n",
        "    # Process sentences\n",
        "    masked_sentences = []\n",
        "    masked_words = []\n",
        "    for sentence in sentences:\n",
        "        masked_sentence, masked_word = randomly_mask_one_word(sentence)\n",
        "        if masked_sentence is not None and masked_word is not None:\n",
        "            masked_sentences.append(masked_sentence)\n",
        "            masked_words.append(masked_word)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({'Fill in the blanks': masked_sentences, 'Answer': masked_words})\n",
        "\n",
        "    # Convert DataFrame to CSV\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "# Example usage:\n",
        "input_file = '/content/test.txt'  # Input text file\n",
        "output_file = 'masked_data_bigger_file.csv' # Output CSV file\n",
        "process_text_file(input_file, output_file)"
      ],
      "metadata": {
        "id": "2rzqQswXODeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This piece of code is used to remove all the line that contain multiple masks\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file into a DataFrame\n",
        "df = pd.read_excel('/content/5000 QnA.xlsx')\n",
        "\n",
        "# Define a function to count occurrences of '[MASK]' in a string\n",
        "def count_mask(string):\n",
        "    return string.count('[MASK]')\n",
        "\n",
        "# Filter the DataFrame to remove rows where 'sentence' column contains two or more '[MASK]'\n",
        "df_filtered = df[df['Fill in the blanks'].apply(count_mask) < 2]\n",
        "\n",
        "# Write the filtered DataFrame back to Excel\n",
        "df_filtered.to_excel('filtered_excel_file.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "deOb2uRAObDn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}