{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Extractive Question Answering on Squad 2.0 dataset using T5-base model.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-19T05:56:10.508623Z","iopub.execute_input":"2024-04-19T05:56:10.509691Z","iopub.status.idle":"2024-04-19T05:56:10.516329Z","shell.execute_reply.started":"2024-04-19T05:56:10.509651Z","shell.execute_reply":"2024-04-19T05:56:10.515266Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Checking if cuda is accessible or not, if not switch to gpu accelerators.\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncuda_available = torch.cuda.is_available()\n\nif cuda_available:\n    print(\"CUDA is available.\")\nelse:\n    print(\"CUDA is not available.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Switching the device to cuda for GPU accelerator\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntorch.set_default_device(\"cuda\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Accessing the model from transformers library\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Set the device to CUDA if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel1_name = \"t5-base\" #the model selected is t5_base from transformers library\ntokenizer1 = T5Tokenizer.from_pretrained(model1_name)\nmodel1 = T5ForConditionalGeneration.from_pretrained(model1_name).to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sample Extractive question and Answer**","metadata":{}},{"cell_type":"code","source":"# Defining the question and context\nquestion = \"why is sky blue,precisely\"\ncontext = \"The sky appears blue due to the scattering of sunlight off the atmosphere. The Earth's atmosphere is composed of various gases and particles. When sunlight hits the atmosphere, shorter blue wavelengths are scattered in all directions by the gases and particles, making the sky appear blue to our eyes.\"\n\n# Formatting the input according to T5's text-to-text format\ninput_text = f\"question: {question} context: {context}\"\n\n# Tokenize inputs\ninputs = tokenizer1(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n\n# Forward pass through the model\nwith torch.no_grad():\n    outputs = model1.generate(**inputs)\n\n# Decode the generated answer\nanswer = tokenizer1.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Extractive QA Answer:\", answer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking the input drives for model and dataset(Squad 2.0)**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json # to read json\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Function to convert squad dataset from json to dataframe**","metadata":{}},{"cell_type":"code","source":"def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n                           verbose = 1):\n    \"\"\"\n    input_file_path: path to the squad json file.\n    record_path: path to deepest level in json file default value is\n    ['data','paragraphs','qas','answers']\n    verbose: 0 to suppress it default is 1\n    \"\"\"\n    if verbose:\n        print(\"Reading the json file\")    \n    file = json.loads(open(input_file_path).read())\n    if verbose:\n        print(\"processing...\")\n    # parsing different level's in the json file\n    js = pd.json_normalize(file, record_path)\n    m = pd.json_normalize(file, record_path[:-1])\n    r = pd.json_normalize(file, record_path[:-2])\n    \n    #combining it into single dataframe\n    idx = np.repeat(r['context'].values, r.qas.str.len())\n    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n    m['context'] = idx\n    js['q_idx'] = ndx\n    main = pd.concat([m[['id','question','context']].set_index('id'), js.set_index('q_idx')], axis=1, sort=False).reset_index()\n    main['c_id'] = main['context'].factorize()[0]\n    if verbose:\n        print(\"shape of the dataframe is {}\".format(main.shape))\n        print(\"Done\")\n    return main","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def squad_json_to_dataframe_dev(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n                           verbose = 1):\n    \"\"\"\n    input_file_path: path to the squad json file.\n    record_path: path to deepest level in json file default value is\n    ['data','paragraphs','qas','answers']\n    verbose: 0 to suppress it default is 1\n    \"\"\"\n    if verbose:\n        print(\"Reading the json file\")    \n    file = json.loads(open(input_file_path).read())\n    if verbose:\n        print(\"processing...\")\n    # parsing different level's in the json file\n    js = pd.json_normalize(file, record_path)\n    m = pd.json_normalize(file, record_path[:-1])\n    r = pd.json_normalize(file, record_path[:-2])\n    \n    #combining it into single dataframe\n    idx = np.repeat(r['context'].values, r.qas.str.len())\n#     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n    m['context'] = idx\n#     js['q_idx'] = ndx\n    main = m[['id','question','context','answers']].set_index('id').reset_index()\n    main['c_id'] = main['context'].factorize()[0]\n    if verbose:\n        print(\"shape of the dataframe is {}\".format(main.shape))\n        print(\"Done\")\n    return main","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creation of Training dataset from squad**","metadata":{}},{"cell_type":"code","source":"# training data\ninput_file_path = '/kaggle/input/squad-20/train-v2.0.json'\nrecord_path = ['data','paragraphs','qas','answers']\ntrain = squad_json_to_dataframe_train(input_file_path=input_file_path,record_path=record_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualising the data and understanding the columns and rows**","metadata":{}},{"cell_type":"code","source":"train['context'][0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Sample extraction and comparison with actual target response**","metadata":{}},{"cell_type":"code","source":"input_text = f\"question: {train['question'][0]} context: {train['context'][0]}\"\n\ninputs = tokenizer1(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n\nwith torch.no_grad():\n    outputs = model1.generate(**inputs)\n\nanswer = tokenizer1.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Extractive QA Answer:\", answer)\nprint(\"Actual QA Answer:\", train['text'][0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Extractive question answering for 50,0000 instances of squad dataset and storing the responses as squad_with_t5_2_csv *************","metadata":{}},{"cell_type":"code","source":"for i in range(1,50000):\n    input_text = f\"question: {train['question'][i]} context: {train['context'][i]}\"\n    inputs = tokenizer1(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = model1.generate(**inputs)\n\n    answer = tokenizer1.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"{i}\",end=\"-\")\n\n    new_entry = {\n        'Context': train['context'][i],\n        'Question': train['question'][i],\n        'Response': train['text'][i],\n        'Response by T5':answer\n    }\n\n    with open('squad_with_t5_2.csv', 'a') as f:\n        pd.DataFrame([new_entry]).to_csv(f, header=f.tell()==0, index=False)","metadata":{},"execution_count":null,"outputs":[]}]}